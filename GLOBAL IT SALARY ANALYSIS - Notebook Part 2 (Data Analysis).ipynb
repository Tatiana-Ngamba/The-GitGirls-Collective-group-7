{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26642f7e-b0b7-471a-8565-57f99747842d",
   "metadata": {},
   "source": [
    "# GLOBAL IT SALARY ANALYSIS (Git-Girls-Collective-7)  \n",
    "# - Notebook Part 2 (Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efdada-ad31-4449-bf4d-f5ce15fb391b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "This notebook is... (explain purpose (data analysis) relative to the previous notebook containing API python code)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73306aaa-c161-4d18-878d-42d81492aa9b",
   "metadata": {},
   "source": [
    "- [ ] Section 1: Transforming API call data into DataFrames\n",
    "- [ ] Section 2: Making a MySQL Database\n",
    "- [ ] Section 3: Loading data from MySQL tables into DataFrames for analysis\n",
    "- [ ] Section 4: Basic Data Analysis\n",
    "- [ ] Section 5: Analysis of Extreme Values\n",
    "- [ ] Section 6: Answering Questions Through Data Analysis\n",
    "- [ ] Section 7: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc1cea-3b7c-4853-9345-6b039b174d04",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Someone may wish to breakdown the contents list above to reflect subsections\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c1d80-6eeb-4e51-b4a9-528a4461de97",
   "metadata": {},
   "source": [
    "# Section 1: Transforming API call data into DataFrames  \n",
    "# (_A loading/cleaning/transforming section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd072d-e5dc-43f6-8516-b3c536dbc5e5",
   "metadata": {},
   "source": [
    "### Notebook Part 2 File Requirements\n",
    "The first three files are the raw datasets we gathered:\n",
    "* **cost_living_w_codes.csv**\n",
    "* **gender_pay_gap.csv**\n",
    "* **country_codes.sql** _- this must be initialised as a DB in MySQL workbench_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff232cfe-be40-4747-8291-aac5ec3f30c9",
   "metadata": {},
   "source": [
    "* **config.py** _- you must complete your MySQL username, password and hostname (see README.md instructions)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fa0de-0bfa-4c8b-86c7-e02e788042f0",
   "metadata": {},
   "source": [
    "* **output_gbp_salaries_23-11-29_10-55.csv**\n",
    "\n",
    "This final file is the output of our main.py Python program: this exact file is the 'frozen' snapshot of API-call data upon which we based our project, timestamped 23-11-29_10-55. If you wish to run this notebook to recreate our analysis based upon the exact same data we did, you must use _the exact file quoted above._\n",
    "\n",
    "Alternatively, you can test our Python program to create your own dataset by running **GLOBAL IT SALARY ANALYSIS - Notebook Part 1 (Python API code).ipynb** to generate a _fresh_ dataset: the csv name will be your equivilent timestamped filename.  The Python program is designed to give the final csv file a unique, timestamped name in the format output_gbp_salaries_{timestamp}, to facilitate version control and data integrity.\n",
    "\n",
    "Our Python program makes several API calls to Teleport for country & salary data, and one to exchangerate-api.com for currency conversion rates.  \n",
    "The resulting file contains the fields: \\* country codes   * local currency code   * salaries in local currency (25th/50th/75th)   * conversion rate to gbp   * gbp converted salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec9f98-5a0d-41bb-a097-db6ce1163e96",
   "metadata": {},
   "source": [
    "### See README.md file for full list of package installation requirements to run this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d13a8-b9c3-4088-bc09-c4d005cbdff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules which are part of the standard Python library\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import required modules and libraries - these may require package install via !pip install <module name>\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Sklearn modules & classes for Machine Learning - these may require package install via !pip install scikit-learn, !pip install statsmodel\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598eeca-3da1-4f32-b8e4-6e8828df6a08",
   "metadata": {},
   "source": [
    "Load the timestamped csv file which has all the combined data from our various API calls, into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cd87e-43ab-4401-b22f-caaf9b44c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_sal_df = pd.read_csv(\"data/output_gbp_salaries_23-11-29_10-55.csv\") # match the filename to the timestamped csv you wish to process\n",
    "api_sal_df.isnull().sum() # Let's check the completeness of our data at import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedb715-6ef2-481b-8803-234595fe5f5b",
   "metadata": {},
   "source": [
    "**Analysis**  52 iso_alpha2 codes are due to a problematic import of the iso_alpha2 code for Namibia which happens to be... \"NA\"! This is a querk of pandas, and will be corrected below. Missing gbp_converted... records are due to Teleport supplying 3 out of date currency codes for three countries (3 countries x 52 jobs is 156), so they therefore did not match with the currency codes used by exchangerate_api. This means we have local currency salary data for these countries, but not GBP converted salaries. These will be cleaned and imputed at SQL database stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962df02-0864-4181-bc54-3a1fa7071598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the missing iso_alpha2, Namibia \"NA\"\n",
    "null_iso_alpha2 = api_sal_df[api_sal_df['iso_alpha2'].isnull()].copy()\n",
    "null_iso_alpha2 # It's Namibia, index 6552-6603\n",
    "api_sal_df.loc[6552:6603, 'iso_alpha2',] = 'NA'\n",
    "api_sal_df.isnull().sum() # fixed: 0 null iso_alpha2 codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec42e7b-a8ca-4064-8eae-fdea67a84f35",
   "metadata": {},
   "source": [
    "### Import data from country_codes.sql, and merge with the API DataFrame   \n",
    "api_sal_df at this point identifies countries only by iso_alpha2 codes, not their names, and we also want to import area and population data from country_codes.sql file.  \n",
    "The following code imports data from the MySQL table and converts it into a pandas DataFrame called \"countries_df\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f9cf5-af97-45cb-98cc-865ddb2222d9",
   "metadata": {},
   "source": [
    "**_Important!_**  \n",
    "You will need to have run country_codes.sql in MySQL Workbench, to have created the \"database countries_db\" and the \"table country_codes\".  \n",
    "You must also have completed your MySQL login credentials within config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f7a41-b949-4082-826e-d78222a3e74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# country_codes.sql information is in an .sql file, which must be converted into a database in order for pandas to turn it into into DF. \n",
    "# Once a DataFrame object, it can be converted to .csv\n",
    "\n",
    "# Import configuration variables from config.py\n",
    "from config import DATABASE_USER, DATABASE_PASSWORD, DATABASE_HOST\n",
    "\n",
    "# MySQl database connection details\n",
    "username = DATABASE_USER\n",
    "password = DATABASE_PASSWORD\n",
    "host = DATABASE_HOST\n",
    "database = 'countries_db'\n",
    "\n",
    "# Creates a database engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\")\n",
    "print(engine)\n",
    "\n",
    "# The SQL query to get all info from table named country_codes\n",
    "query = \"SELECT * FROM country_codes\"  \n",
    "\n",
    "# Use Pandas to load data into a DataFrame\n",
    "countries_df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbb204-cc85-4c7e-8f6d-cdfd37e31a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries_df.head() # preview the imported DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d70985-dfc8-444f-b6ee-8def4f74b519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell if you'd like to view an intermediate backup of the DataFrame in .csv format\n",
    "# countries_df.to_csv(\"country_codes.csv\", encoding='utf-8', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0aba3-8042-4efb-93b1-b9dfcfc052b1",
   "metadata": {},
   "source": [
    "Join api_sal_df to countries_df. This is an inner join because we don't need info about countries for which we have no salary data (that being the focus of our data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb42efb-443c-4d37-ad96-3cae0c6d01fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inner join api_sal_df to country_codes df on iso_alpha2\n",
    "biggie_dfv1 = pd.merge(api_sal_df, countries_df, on='iso_alpha2', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eeab99-cb89-4e24-9c14-40218c1509fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if you'd like to view an intermediate backup of the DataFrame in .csv format\n",
    "# biggie_dfv1.to_csv(\"sal_and_country.csv\", index=False) # intermediate backup of DF to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7221dd-83a1-4140-a0fc-514140dc6fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biggie_dfv1.head(10) # preview the merged DF. [10296 rows x 19 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc51f23-28e8-4c35-87fc-cd098c13846e",
   "metadata": {},
   "source": [
    "### Join the gender pay parity data (a column from gender_pay_parity.csv) with our growing DF. Read to DF, clean, merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48122caa-c34a-4b7d-a6c2-cc92bbc18412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data in gender_pay_parity.csv column into a new DataFrame\n",
    "gender_df = pd.read_csv(\"data/gender_pay_gap.csv\")\n",
    "gender_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfa80c-aeeb-4873-8ae4-80602335b6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleaning: rename coumns Country to country / Gender_Pay_Parity gender_pay_parity to facilitate merge\n",
    "gender_df.rename(columns={'Country': 'country', 'Gender_Pay_Parity':'gender_pay_parity'}, inplace=True)\n",
    "gender_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d4eb9-62e5-4d95-84f5-d42829d0df0f",
   "metadata": {},
   "source": [
    "### Join our growing DF with gender_pay_parity column data  \n",
    "This is an _outer_ join because we are joining on the 'country'_name_ column rather than a controlled, standardised column like iso_alpha2. If this wasn't an outer join, we may miss data which doesn't match due to minute different spellings of names. The data is reviewed later, as SQL database creation stage, for duplicate countries (same country but slightly different names/different characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2154a9-11c8-4a35-8a54-f29ad362722a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biggie_dfv2 = pd.merge(biggie_dfv1, gender_df, on='country', how='outer') \n",
    "# biggie_dfv2.to_csv(\"sal_and_country_and_gender.csv\", index=False) # intermediate backup of DF to .csv file\n",
    "biggie_dfv2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6619f2e-55d9-49f9-adb3-188c5f910abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv2.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91cc9a-6a32-4237-8afb-8830da5fcbd8",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "We now have quite a few nulls now (18 iso_alpha2 codes and 18 _more_ nulls in our gbp_converted salary columns) than previous. This is due to the outer join, with rationale outlined above. These will be countries from the country_codes table for which we have no salary data or matching iso_alpha2 code from Teleport. These will be cleaned at the SQL database stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51ea1c-9144-49ab-a24b-3e89424cd952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_countries = biggie_dfv2['country'].unique().tolist()\n",
    "count_countries = len(unique_countries)\n",
    "print(f\"We have gathered information on {count_countries} distinct countries: {count_countries-198} have been imported, even though we do not have salary data for them,\\nthrough the outer join with the external dataset country_codes.sql. This means we have preserved full data on the 198 countries\\nwe imported salary information for through Teleport's API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccd53d-ae21-49d8-91e4-fa960bd940c4",
   "metadata": {},
   "source": [
    "### Final join for our growing DF with columns from cost_of_living.csv data (from WorldData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abad1d-607e-4075-8a07-64042bd87943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, load the cost_of_living.csv data into a DataFrame\n",
    "cost_living_df = pd.read_csv(\"data/cost_living_w_codes.csv\")\n",
    "cost_living_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e02665-2314-4bac-92f6-39824789cef6",
   "metadata": {},
   "source": [
    "Then merge cost_of_living DF with our current main DataFrame (biggie_dfv2), to create a final superlarge DataFrame containing all data. The left join treats our 'main' DF as the master, importing only cost_of_living data on those countries we have salary data for (avoiding adding anymore nulls). This form of inner join is supported by the fact that we are joining on a controlled, normalised column iso_alpha2 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337f4ae-39dd-4182-9c0e-d9beece3bada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biggie_dfv3 = pd.merge(biggie_dfv2, cost_living_df, on=\"iso_alpha2\", how=\"left\") \n",
    "biggie_dfv3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ee49d-c278-43f8-afc5-6d14b27f275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv3.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc189e9-7bb1-45ef-a847-3f83c45e2371",
   "metadata": {},
   "source": [
    "**Analysis**: We haven't added anymore superflous countries (represented by null iso_alpha2 codes) in this last DataFrame merge, however we can see that there are quite a few countries of the 198 for which we don't have a gender_pay_parity metric and / or the cost_of_living metrics. This is to be expected when merging independent datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547fbe0-7eb9-40d5-a488-260210b4123f",
   "metadata": {},
   "source": [
    "### Cleaning: rename the column 'rank' from cost_of_living.csv to WD_cost_living_rank for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab38c3f-faa4-4844-9ddc-119c4c97aca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biggie_dfv3.rename(columns={'rank': 'WD_cost_living_rank', 'country_or_region': 'WD_country_or_region'}, inplace=True)\n",
    "biggie_dfv3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849eb4d-9c27-411a-810d-75ed2fe414bd",
   "metadata": {},
   "source": [
    "### Cleaning: rename local currency columns (named by Teleport API) to make shorter and clearer that values are in local currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f979d-8bfa-4133-b918-dd8cecd80e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biggie_dfv3.rename(columns={'salary_percentiles_percentile_25': 'salary_local_25th_pcl', 'salary_percentiles_percentile_50': 'salary_local_50th_pcl', 'salary_percentiles_percentile_75': 'salary_local_75th_pcl', 'monthly_income_USD' : 'WD_monthly_income_USD', 'notes_special_regions' : 'WD_notes_special_regions'}, inplace=True)\n",
    "# biggie_dfv3.to_csv(\"sal_country_gender_costliving.csv\", index=False) # intermediate backup of DF to .csv file\n",
    "biggie_dfv3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942f861-1258-4945-b874-2c973c1af1b4",
   "metadata": {},
   "source": [
    "**Explanation** The salary data from Teleport was provided in each country's local currency. We planned to relativise the salaries for comparison and analysis by converting them to GBP at today's rates (all built into our dynamic Python API program). We realise also though that these figures are not directly comparable, becuase of the varying costs of living in each country. This is why we gathered the cost of living metrics from WorldData. However, we cannot use their metrics directly, because their cost of living rankings and metrics are all given in USD. So, we require another stage of conversion to relativise this data: converting the USD_monthly_income column into GBP. The following function calculates and adds another column to the DataFrame to provide this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4b1b7-4d98-4366-9bae-edbfdd325299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate and add new column for GBP monthly income\n",
    "def usd_monthly_income_to_GBP(USD_monthly_income):\n",
    "    if isinstance(USD_monthly_income, str) and USD_monthly_income.strip():\n",
    "        USD_num_only = USD_monthly_income.replace(\",\", \"\").replace(\" USD\",\"\").strip() # string formatting to remove USD and leave a viable integer\n",
    "    else:\n",
    "        return None # if not a string. note, no print message, should just skip the entry\n",
    "    try: \n",
    "        USD_num_only = float(USD_num_only)\n",
    "    except ValueError:\n",
    "        print(\"Error converting string to int\") \n",
    "        return None\n",
    "    GBP_monthly_income = int(USD_num_only * 1.267997) # exchange rate for USD > GBP on 29th Nov when output_gbp_salaries_23-11-29_10-55.csv was produced\n",
    "    return GBP_monthly_income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243d7ef-d73a-4f81-9008-78416fd93069",
   "metadata": {},
   "source": [
    "Create a final DataFrame with country average monthly salary (WorldData) converted to GBP in new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbfc89-1fde-4683-9202-b3f770c6ef16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = biggie_dfv3.copy() # leave biggie_dfv3 intact\n",
    "final_df['WD_monthly_income_GBP'] = final_df['WD_monthly_income_USD'].apply(usd_monthly_income_to_GBP)\n",
    "# reordering the columns for the final DF\n",
    "final_df = final_df[\n",
    "    [\n",
    "        'iso_alpha2',\n",
    "        'country',\n",
    "        'currency_code',\n",
    "        'local_to_gbp_rates',\n",
    "        'job_id',\n",
    "        'job_title',\n",
    "        'salary_local_25th_pcl',\n",
    "        'gbp_converted_25th',\n",
    "        'salary_local_50th_pcl',\n",
    "        'gbp_converted_50th',\n",
    "        'salary_local_75th_pcl',\n",
    "        'gbp_converted_75th',\n",
    "        'WD_country_or_region',\n",
    "        'WD_cost_living_rank',\n",
    "        'WD_monthly_income_USD',\n",
    "        'WD_monthly_income_GBP',\n",
    "        'WD_notes_special_regions',\n",
    "        'cost_index',\n",
    "        'purchasing_power_index',\n",
    "        'gender_pay_parity',\n",
    "        'iso_alpha3',\n",
    "        'iso_numeric',\n",
    "        'fips',\n",
    "        'capital',\n",
    "        'area_km2',\n",
    "        'population',\n",
    "        'continent'\n",
    "    ]\n",
    "]\n",
    "final_df.to_csv(\"final_df_inc_GBP_monthly.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5abf7e-0edb-42b6-8bf7-6d529d73fc94",
   "metadata": {},
   "source": [
    "**Analysis** The purpose of this 'superlarge' dataframe - at this stage - is not to have fully _saturated_ data, but to represent fully _combined_ data, with all the redundancy this brings. final_df is the biggest possible combination of all our datasets, providing the opportunity from here to refine down according to columns that are relevant for particular data analysis questions. From here we can select cross-referenced columns to streamline into DataFrames tailored to individual topics of interest. Having a large \"data lake\" gives us the greatest scope from which to refine subsets of data to address specific questions. Should avenues of enquiry arise which we didn't foresee at the outset, final_df will be the point to which the group can roll back to potentially refine our data analysis along different paths. In this way it is a milestone in the flow of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373786e4-59ca-49bc-b6d1-a14b21afedc0",
   "metadata": {},
   "source": [
    "# Section 2 - Making a MySQL Database (_A loading/cleaning/transforming section_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108253db-e2a4-469e-8239-a56fd052a7df",
   "metadata": {},
   "source": [
    "Splitting the large DF into 4 refined DataFrames to assist with populating MySQL Database tables.  \n",
    "Only run this cell if you want individual copies of the csvs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd14fe-e4f5-4818-bfa5-17e02581fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a directory to keep these csvs separate\n",
    "if not os.path.exists(\"dfs_for_db\"):\n",
    "        os.makedirs(\"dfs_for_db\")\n",
    "\n",
    "# Create reduced DataFrames to serve as SQL table starters\n",
    "countries_sql_table_df = final_df[['iso_alpha2', 'country', 'capital', 'continent', 'area_km2', 'population','gender_pay_parity']].drop_duplicates(subset='iso_alpha2').copy() # excluded 'iso_alpha3', 'iso_numeric' 'fips' \n",
    "countries_sql_table_df.to_csv(\"dfs_for_db/countries_data_from_final_df.csv\", index=False)\n",
    "\n",
    "cost_of_living_sql_table_df = final_df[['iso_alpha2', 'WD_country_or_region','WD_notes_special_regions','WD_cost_living_rank', 'WD_monthly_income_USD', 'WD_monthly_income_GBP', 'cost_index', 'purchasing_power_index']].drop_duplicates(subset='iso_alpha2').copy()\n",
    "cost_of_living_sql_table_df.to_csv(\"dfs_for_db/cost_of_living_data_from_final_df.csv\", index=False)\n",
    "\n",
    "salaries_sql_table_df = final_df[['iso_alpha2',  'job_id', 'job_title', 'salary_local_25th_pcl', 'salary_local_50th_pcl', 'salary_local_75th_pcl', 'currency_code', 'local_to_gbp_rates','gbp_converted_25th','gbp_converted_50th', 'gbp_converted_75th']].copy()\n",
    "salaries_sql_table_df.to_csv(\"dfs_for_db/salaries_data_from_final_df.csv\", index=False)\n",
    "\n",
    "job_sql_table_df = final_df[['job_id', 'job_title']].drop_duplicates(subset='job_id').copy()\n",
    "job_sql_table_df.to_csv(\"dfs_for_db/job_data_from_final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cda717-346e-43bf-8d3d-39e50d1490eb",
   "metadata": {},
   "source": [
    "### **The process of creating the SQL database involved the following steps:**\n",
    "\n",
    "1. We began by utilising API data along with country codes and gender disparity information to construct a comprehensive spreadsheet. This spreadsheet had distinct sheets representing both the SQL structure tables and sample data.\n",
    "2. To enhance clarity, we organised the data into various sheets, facilitating the visualization of SQL tables alongside their corresponding sample data.\n",
    "3. Subsequently, an Entity-Relationship (ER) diagram was developed to provide a visual representation of the tables, columns, and data types. Initially, around 10 different tables were conceptualised based on the original datasets.\n",
    "4. In a collaborative group meeting, the ER diagram was presented, fostering discussions that led to the finalisation of the SQL tables.\n",
    "5. The conclusive version of the ER diagram incorporated essential details such as primary and foreign keys, ensuring the integrity of the relational database.\n",
    "6. Following this, we returned to the Excel spreadsheet to normalize the database. Three distinct sheets were created, each intended to serve as an SQL table.\n",
    "7. We meticulously transferred the data into the corresponding columns, and to ensure accuracy, references such as iso_alpha2 and iso_alpha3 codes were cross-referenced with the additional data in adjacent columns.\n",
    "8. To streamline the process, we used the V-LOOKUP formulas. These formulas were instrumental in accurately identifying country codes and gender disparities for each country, thereby enhancing the overall coherence and reliability of the dataset\n",
    "\n",
    "### Countries Table:\n",
    " \n",
    "1. Out of the 250 countries considered, gender pay disparity data was available for 136 countries. For the remaining countries, 'NULL' values were populated in the SQL database to replace the absence of this information.\n",
    "2. Four specific **countries—Antarctica**, **Bouvet Island**, **Heard and McDonald Islands**, and **U.S. Outlying Islands** had '0' as the recorded value for their population. Additionally, for U.S. Outlying Islands, there was a recorded '0' for the area in square kilometers.\n",
    "3. Antarctica presented a unique case with 'N/A' as the recorded value for Currency Code and this was treated as 'NULL' in the SQL database.Upon further investigation, it was revealed that Antarctica is a continent without a native population. However, it is home to a transient population of scientists and support staff from various countries who live and work in research stations. Given the absence of adefined population figure, '0' was used to represent this value.\n",
    "4. **Bouvet Island**, situated in the South Atlantic Ocean and under Norwegian dependency, is uninhabited,justifying the '0' population value. Similarly, **Heard and McDonald Islands**, administered by Australia for scientific research purposes, also have no permanent population.\n",
    "5. **U.S. Outlying Islands**, a group of nine insular areas outside the 50 states and the District of Columbia, exhibited varied population statuses—some with small populations and others uninhabited. This diversity in population characteristics was reflected in the SQL database for accurate representation5\n",
    "6. We have the gender pay for **Congo Republic** but not for **DR Congo**. after further research, we realised that, **Democratic Republic of the Congo (DRC)** is the larger of the two countries and is often referred to simply as the Congo and Its capital is Kinshasa. Whereas **Republic of the Congo** is a separate, neighbouring country sometimes referred to as Congo-Brazzaville to distinguish it from the Democratic Republic of the Congo.\n",
    "\n",
    "### Job Table:\n",
    "1. We didn’t experience any issues with this tables as the data was similar to the other tables.\n",
    "\n",
    "### Salaries Table:\n",
    "1. Incorrect values were displayed for an accountant in Ghana(GH). Please see below the incorrect values displayed  \n",
    "**|1|GH|ACCOUNTANT|Accountant|1|£0.065696677|2|£0.131393354|4|£0.262786709O3**\n",
    "2. We were missing the converted salary rate for the ‘percentile_25_GBP, percentile_50_GBP and percentile_75_GBP’. These values were missing for iso_alpha2 - ‘VE’, ‘MR’ and ‘BY'\n",
    "3. Revisiting the API code and currency_rates.json we noticed that the three countries did have currency conversion rates, but that the currency code attached to their Teleport data entry was out of date, hence why the converted salary figures were not pulled through. \n",
    "4. The updated currency code and the GBP converted salary values were updated in excel for each relevant country before inputting it in SQL.\n",
    "\n",
    "\n",
    "### Exporting the data from SQL tables into job_insights.xlsx involved the following steps:\n",
    "1. We decided that it would be great to have a spreadsheet where all the tables data were displayed in one spreadsheet as it would be easier to read different sheets from Jupyter Notebook and for pandas DataFrames to be converted directly from these.\n",
    "2. **Countries Table** - We wanted to be able to export this data from MySQL as a csv file and this data was later pasted into **job_insights.xlsx**.\n",
    "3. **Salaries & Job Table** - Exporting data from these tables were slightly difficult as SQL only allowed us to export 1000 rows of data. The salaries and job tables has about 10,000 rows of data. We exported the data in order by numbering each CSV file. The data from the CSV files were then pasted into **job_insights.xlsx**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6200a25-40ba-40dc-98b8-5815aa917756",
   "metadata": {},
   "source": [
    "### To view the MySQL database that was created to normalise, structure and hold our data, please see the file **job_market_insights_database.sql** in the /data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed51137-3bd1-4cc1-b7f3-cfbcbadf0d2a",
   "metadata": {},
   "source": [
    "# Section 3 - Loading data from MySQL tables into DataFrames for analysis.  \n",
    "# _(A loading/cleaning/transforming section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c9320-4496-4f0c-b136-86612bcd8ed7",
   "metadata": {},
   "source": [
    "### Notebook File Requirements for Section 3\n",
    "* **job_insights.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4059643-69c6-40b5-a982-5929363ad95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Countries table only\n",
    "SQL_countries_df = pd.read_excel('data/job_insights.xlsx', sheet_name = 'Countries')\n",
    "SQL_countries_df.isnull().sum() # The SQL database had full saturation of iso_alpha2 codes, however one shows missing here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fea441-5223-4bb2-9b14-9884bed2da57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct the missing iso_alpha2. This was a PK in the SQL DB, so must have been present but lost in conversion to DF. This is a pandas qwerk with the value \"NA\"\n",
    "null_iso_alpha2 = SQL_countries_df[SQL_countries_df['iso_alpha2'].isnull()]\n",
    "null_iso_alpha2 # It's Namibia, index 159\n",
    "SQL_countries_df.loc[159, 'iso_alpha2'] = 'NA'\n",
    "SQL_countries_df.isnull().sum() # fixed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5502732-760f-45b7-b886-caf2923763f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_currency_code = SQL_countries_df[SQL_countries_df['currency_code'].isnull()]\n",
    "null_currency_code  # It's Antarctica, index 8\n",
    "# Replace DF with DF keeping only the rows where iso_alpha2 is not 'AQ'\n",
    "SQL_countries_df = SQL_countries_df[SQL_countries_df['iso_alpha2'] != 'AQ']\n",
    "SQL_countries_df.isnull().sum()  # fixed. So 113 countries with no gender_Pay info, and 41 without a continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbdba2-50c1-477a-9086-264ca1da5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salaries table only\n",
    "SQL_salaries_df = pd.read_excel('data/job_insights.xlsx', sheet_name = 'Salaries')\n",
    "SQL_salaries_df.isnull().sum() # Namibia causing problems again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96834d-1f64-446f-9658-0395f0d008c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Namibia's \"NA\" iso_alpha2 code from NaN values (lost in pandas DF creation) to the country code NA\n",
    "SQL_salaries_df.loc[6553:6604, 'iso_alpha2'] = 'NA'\n",
    "SQL_salaries_df.iloc[6553:6604]\n",
    "SQL_salaries_df.isnull().sum() # all 0, so all columns complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32920cb-d70c-45be-bdae-a048a26018d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SQL_all_df = pd.merge(SQL_salaries_df, SQL_countries_df, on='iso_alpha2', how='inner')\n",
    "SQL_all_df.to_excel(\"SQL_all_data_joined_inner.xlsx\", index=True)\n",
    "SQL_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74d745-fc3d-4257-88ae-2fcb75a5c0e0",
   "metadata": {},
   "source": [
    "# Section 4 - Basic Data Analysis _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e687ba-72ad-4f64-960f-fc8b3269c2fb",
   "metadata": {},
   "source": [
    "Some basic stats about our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e13011-f697-4a7e-8661-d4e202d21ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries_list = SQL_all_df['iso_alpha2'].unique().tolist() \n",
    "countries_count = len(countries_list) \n",
    "countries_count # 198 unique country codes. Matches with number of countries Teleport provided salary data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6306f0c-3126-421b-b219-cf1a8ba9d59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobs_list = SQL_all_df['Job_id'].unique().tolist() \n",
    "jobs_count = len(jobs_list) \n",
    "jobs_count #  52 unique jobs. Matches the Teleport job range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bffb70-f7a3-4afa-948e-0735ca937b16",
   "metadata": {},
   "source": [
    "## Section 4.1 - Missing Values Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f93c8-ee4c-4183-9319-3ca505a3d521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Heatmap of the null values\n",
    "plt.figure(figsize = (8,6))\n",
    "ax = sns.heatmap(SQL_all_df.isnull(), cmap = 'viridis', cbar=False, annot=False)\n",
    "\n",
    "# Absolute totals of missing values for annotations for columns\n",
    "tot_gender_pay_missing = SQL_all_df['gender_Pay'].isnull().sum()\n",
    "tot_continent_missing = SQL_all_df['continent'].isnull().sum()\n",
    "\n",
    "# Positioning the absolute total labels\n",
    "n = list(SQL_all_df.columns).index('gender_Pay')\n",
    "m = list(SQL_all_df.columns).index('continent')\n",
    "ax.text(n+0.5, -0.5, tot_gender_pay_missing, ha='center', va='bottom', color='green', fontsize=15) \n",
    "ax.text(m+0.5, -0.5, tot_continent_missing, ha='center', va='bottom', color='green', fontsize=15) \n",
    "\n",
    "# legend\n",
    "plt.text(0.02, 0.75, 'Green Text = Total Missing Values', color='green', fontsize=15, transform=ax.transAxes)\n",
    "\n",
    "ax.set_title('Missing Data Heatmap', pad=20, fontsize=15)\n",
    "plt.ylabel('Entries Missing')\n",
    "plt.xlabel('Data Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf21b1a-b72f-44bb-ba04-de72165c32b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_gender_pay = int(tot_gender_pay_missing / jobs_count)\n",
    "missing_gender_pay # 69\n",
    "print(f\"There are {missing_gender_pay} countries without gender pay parity figures out of {countries_count}, which is {(missing_gender_pay/countries_count)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b67aac-7918-4528-a6b7-ba360931861d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Due to our data processing choices and the nature of our joins, our data is fully saturated for the series we are most interested in (country, salaries in GBP).\n",
    "* Missing continent data could be fairly easily updated, however given the time alloted for our analysis we are unlikely to be able to branch out into broadeer factors like geography, continent etc.\n",
    "* We do not have full gender pay parity figures for all countries, only 66% of our total dataset. However, that does still leave 129 countries with complete data, which means it is likely to be worth analysing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ff633-ec8b-4337-b3f1-2b7dd97d21a2",
   "metadata": {},
   "source": [
    "## Section 4.2 - IT Salaries, Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56d0f0-f2da-49f9-8311-37acaf030aeb",
   "metadata": {},
   "source": [
    "These are the IT roles the group decided we wanted to focus our analysis on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faff3a-c8f9-45ff-9418-b7edc9ffcdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "it_roles = ['BUSINESS-ANALYST', 'DATA-ANALYST', 'DATA-SCIENTIST', 'IT-MANAGER', 'MOBILE-DEVELOPER', 'PRODUCT-MANAGER', 'QA-ENGINEER', 'SOFTWARE-ENGINEER', 'UX-DESIGNER', 'WEB-DESIGNER', 'WEB-DEVELOPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac90976-07db-4cc6-8754-5527bf640ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generating a DF with those specified it_roles\n",
    "it_roles_df = SQL_all_df[SQL_all_df['Job_id'].isin(it_roles)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8963e-61cf-442a-b75f-308e206336ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate a box plot for the countries with their GBP converted salaries using the 50% percentile\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(it_roles_df['percentile_50_GBP'], vert=False)\n",
    "plt.title('Box Plot of Salary for IT roles')\n",
    "plt.xlabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c3269-9af1-4338-8a7a-fbfd1401278a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a histogram for the same data to see the distribution of the IT salaries per country, with a kernel density estimate (trend line)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(it_roles_df['percentile_50_GBP'], bins=15, kde=True, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Histogram with Kernel Density Estimate (KDE) for Salary Percentiles (Percentile 50)')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26652f2-1101-4a7c-93e2-3c611d86f841",
   "metadata": {},
   "source": [
    "### Plots Interpretation\n",
    "On this first analysis about the validity of the data in relation to the IT salaries, from both plots above we can conclude that most of the salaries are in a range between 0 to 20K GBP. The box plot is in fact clearly right skewed and for further analysis we might just consider the salaries below 70K GBP. Considering that the countries analysed have different cost of living it is expected to see a wide distribution. To deep dive further on this and identify other possible outliers in the next section we display box plots per country, rather than all the data in one single plot as the salary distribution inside the same country should be more normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c57a87-7239-49cf-b604-9c9defea96e6",
   "metadata": {},
   "source": [
    "## Section 4.3 - Gender Pay Parity, Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa230235-eac2-4dca-9c08-929b1fea16f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a histogram to analyse the pay parity values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(SQL_all_df['gender_Pay'], bins=15, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of gender pay parity')\n",
    "plt.xlabel('Wage gap')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4285edf-539f-4a07-9171-1c3ba6f05642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the following code to see that the outlier value corresponds to Afghanistan.\n",
    "top_lowest_countries = SQL_all_df.nsmallest(n=5, columns='gender_Pay')\n",
    "top_lowest_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c67480-25c9-4e73-8b33-678937b53198",
   "metadata": {},
   "source": [
    "Therefore, when carrying out further analysis in regards to pay parity, Afghanistan value must be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad15495-8066-4e3d-bbab-92e3df4e4a38",
   "metadata": {},
   "source": [
    "### Plot interpretation\n",
    "The world pay parity index lies between 0.7 and 0.8. Thanks to the histogram we detected a clear outlier in our pay parity data related to Afghanistan as it is far from the histogram distribution. For the following analysis using the pay parity paramenter we will not take Afghanistan pay parity value into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbd4cf-31e8-4bde-bbc6-bbbaa443c883",
   "metadata": {},
   "source": [
    "# Section 5 - Analysis of Extreme Values _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53717c6-2ce0-4382-814c-822791d7d5e3",
   "metadata": {},
   "source": [
    "### Section 5.1 - A Deep Dive into Potential Salary Outliers within IT roles as a group, across countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39597f-7194-4424-b51c-8e692343f92e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_countries = it_roles_df['country_name'].unique()\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_countries = len(unique_countries)\n",
    "num_cols = 4  # Number of columns in each row\n",
    "num_rows = math.ceil(num_countries / num_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 3 * num_rows))\n",
    "fig.suptitle('Box Plots for Percentile 50 GBP by Country', y=1.02)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through unique countries and create box plots\n",
    "for i, country in enumerate(unique_countries):\n",
    "    # Filter the DataFrame for the specific country\n",
    "    country_data = it_roles_df[it_roles_df['country_name'] == country]\n",
    "    \n",
    "    # Create a box plot for the percentile_50_GBP column\n",
    "    sns.boxplot(x='percentile_50_GBP', data=country_data, ax=axes[i])\n",
    "    \n",
    "    # Set title and labels\n",
    "    axes[i].set_title(f'Box Plot for {country}')\n",
    "    axes[i].set_xlabel('Percentile 50 GBP')\n",
    "    axes[i].set_ylabel('')\n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Please be patient: this generates a box plot for each of our 11 roles across all the available countries, so it can take a few seconds to load!\n",
    "# You may wish to right-click and 'Enable Scrolling for Outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da08387-c7f7-4b5e-be4f-f0c14dffcc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_index_per_country = it_roles_df.groupby('country_name')['percentile_50_GBP'].idxmax()\n",
    "\n",
    "# Extract the corresponding columns\n",
    "columns_of_interest = ['Job_id', 'percentile_50_GBP', 'country_name']\n",
    "highest_gbp_rows = it_roles_df.loc[max_index_per_country, columns_of_interest]\n",
    "\n",
    "# Display the result\n",
    "print(highest_gbp_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fd2bb-280b-4321-abc1-3cd59ed9af5c",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "In the box plots per country we see the data is more normalized, but still we often still observe an outlier per country on the upper side. After extracting a sample of the highest paying jobs per country according to the Dataset used, it shows us that the Data Scientists tend to earn more. It is important to take into consideration that only 11 IT roles were part of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e70c5d-054c-4f66-abc3-c87011db4b53",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Decision to make - do we need Section 5.2 or has it been replaced by Alicia's boxplots? To delete or complete!\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc37e5-e144-481a-895c-3f6981b90a16",
   "metadata": {},
   "source": [
    "### Section 5.2 - A Deep Dive into IT Role Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dcc20-0893-4cc2-8aa1-64cb83276e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for full integrity of DataFrame used for next analysis\n",
    "SQL_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc8f4c-c855-4db7-80db-58e7c9bb255e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell takes a single job role (data analyst) and looks at it's GBP converted salaries across all countries\n",
    "X = np.where(SQL_all_df['Job_id'] == 'DATA-ANALYST')\n",
    "df = SQL_all_df.iloc[X]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb439a-87ec-48b6-84c6-1538b174e0b9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of what this table demonstrates (i.e. problematic variation in salaries for same job role across countries, even  though converted to GBP, and even after you roughly account for varying costs of living\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa7a6f-9f90-4df1-abf5-61b056546c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This graph needs a title, it's a boxplot of the 50th percentile salaries for Data Analyst role\n",
    "sns.boxplot(df['percentile_50_GBP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbf115-652c-47b2-bf2f-2a6df1989b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(df[\"percentile_50_GBP\"], kde=True, stat=\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e13ccc-0c4b-4dd4-9fbe-2a2afc78a0d1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of histogram needed\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b3128-1724-4e31-962a-4352d37a13ee",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "We need a cell that explains how we have adapted our analysis, based on the above demonstration of extreme values, to show something meaningful from what we have (based upon the assumption that WITHIN COUNTRIES salary ranges are realistic/consistent)... \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbb265-6c4c-425b-9db5-f25515b8ed0d",
   "metadata": {},
   "source": [
    "## Section 6 - Answering Questions Through Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366188a4-d9e8-4afb-bc46-28f71434fa48",
   "metadata": {},
   "source": [
    "## Section 6.1 - How are IT roles paid in comparison to the country medians?\n",
    "_Comparing IT Roles Salaries Across Countries_  \n",
    "\n",
    "The following cell defines and calls a function which takes in a list of interesting job_ids (IT roles) and produces bar charts for each role showing how the salary for that role compares to the median 50th percentile GBP salary across all the countries for which we have data. This is intended to give an idea of how well paid that role is, relative to others, globally, whilst respecting the limitations we found within our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aa9bc-71a5-4b87-996b-ef30018b5ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def median_placement(job):\n",
    "    data = SQL_all_df\n",
    "    iso_alpha2 = data['iso_alpha2'].unique()\n",
    "    more=[]\n",
    "    less=[]\n",
    "    equal=[]\n",
    "    \n",
    "    for x in range(len(iso_alpha2)):\n",
    "        it = np.where(data['iso_alpha2'] == iso_alpha2[x])\n",
    "        it = it[0]\n",
    "        df = data.iloc[it]\n",
    "        \n",
    "        med = df['percentile_50_GBP'].median()\n",
    "        r_med = math.ceil(med*100)/100\n",
    "        #print(r_med)\n",
    "        data_analyst = df.iloc[np.where(df['Job_id'] == job)]\n",
    "        salary = data_analyst['percentile_50_GBP']\n",
    "        r_salary = math.ceil(salary.item()*100)/100 \n",
    "        #print(r_salary)\n",
    "        \n",
    "        if r_salary < r_med:\n",
    "           # print(data_analyst['iso_alpha2'].item())\n",
    "           # print('less than median\\n')\n",
    "            less.append(salary.item)\n",
    "\n",
    "        elif r_salary > r_med:\n",
    "          #  print(data_analyst['iso_alpha2'].item())\n",
    "           # print('more than median\\n')\n",
    "            more.append(salary.item)\n",
    "\n",
    "        elif r_salary == r_med:\n",
    "           # print(data_analyst['iso_alpha2'].item())\n",
    "            #print('same as median\\n')\n",
    "            equal.append(salary.item)\n",
    "\n",
    "        else:\n",
    "            print(data_analyst['iso_alpha2'].item())\n",
    "            print('error')\n",
    "    print(job, '-', 'less:', len(less), 'equal:', len(equal), 'more:',len(more))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6702b4-c1c2-49c3-afe1-a719b8abba72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "it_jobs = ['BUSINESS-ANALYST', 'DATA-ANALYST', 'DATA-SCIENTIST', 'IT-MANAGER', 'MOBILE-DEVELOPER', 'PRODUCT-MANAGER', 'QA-ENGINEER',\n",
    "           'SOFTWARE-ENGINEER', 'UX-DESIGNER', 'WEB-DEVELOPER', 'WEB-DESIGNER']\n",
    "for x in range (len(it_jobs)):\n",
    "    median_placement(it_jobs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fad4f-8de9-4b56-a133-edb0ee22a362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = it_jobs\n",
    "more = [171, 34, 182, 188, 177, 188, 36, 182, 144, 34, 13]\n",
    "equal = [2, 0, 0, 0, 0, 0, 1, 0, 14, 0, 0]\n",
    "less = [25, 164, 16, 10, 21, 10, 161, 16, 40, 164, 185]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bar_width=0.5\n",
    "# Plotting each category as a stacked bar\n",
    "ax.bar(categories, less, label='Less than Median', color='blue', width=bar_width)\n",
    "ax.bar(categories, equal, bottom=[i+j for i,j in zip(less, more)], label='Equal to than Median', color='darkorange', width=bar_width)\n",
    "ax.bar(categories, more, bottom=less, label='More than Median', color='powderblue', width=bar_width)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Number of Countries')\n",
    "ax.set_title(\"Stacked Bar Chart of IT jobs compared to their Country's Median Salary\")\n",
    "ax.legend(loc='center left')\n",
    "ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4cadb-b4d5-4375-9c17-1c756ae6223f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Analysis\n",
    "\n",
    "IT jobs where the majority of countries pay **more than the median salary**:\n",
    "- Business Analysts\n",
    "- Data Scientists\n",
    "- IT Managers\n",
    "- Mobile Developers\n",
    "- Product Managers\n",
    "- Software Engineers\n",
    "- UX Designers\n",
    "\n",
    "\n",
    "IT jobs where the majority of countries pay **less than the median salary**:\n",
    "- Data Analysts\n",
    "- QA Engineers\n",
    "- Web Designers\n",
    "- Web Developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e3b6-b853-4065-a0df-33109184dbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyst=np.where(SQL_all_df['Job_id']=='DATA-ANALYST')\n",
    "data=SQL_all_df.iloc[analyst]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c57caa-ed52-4540-9852-b2ea5c527786",
   "metadata": {},
   "source": [
    "### Plot Interpretation\n",
    "Data Analysts, QA Engineers, Web Developers and Web Designers stand out as occupations where the salary is very likely to be compensated below the country’s median salary.\n",
    "Business Analysts, Data Scientists, IT Managers, Mobile Developers, Product Managers, Software Engineers and UX Designers stand out as occupations where the salary is very likely to be compensated above the country’s median salary.\n",
    "Another interesting observation is the contrast between the extremes, and that very few countries have IT salaries that are exactly at the median, indicating a clear divide between higher and lower-paying roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f26d55-0dcf-481b-9441-74a5307b2d22",
   "metadata": {},
   "source": [
    "## Section 6.2 - How do the salaries of Data Analysis compare to those of Software Engineers?\n",
    "_Comparing Salaries for Two Sample Jobs, Relative to the Median Salary per Country_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2abe7-d200-4a5c-9601-db77b034ec50",
   "metadata": {},
   "source": [
    "The following function compares the salaries for two job roles highly relevant for CFG Degree students: Data Analysts and Software Engineers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77102540-98d6-42ce-b60d-fde1e80b36cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_values = ['Data Analyst', 'Software Engineer']\n",
    "data1 = [173, 16]\n",
    "data2 = [30, 182]\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "positions1 = np.arange(len(x_values))\n",
    "positions2 = positions1 + bar_width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(positions1, data1, width=bar_width, label='Less than Median')\n",
    "ax.bar(positions2, data2, width=bar_width, label='More than Median')\n",
    "\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(positions1 + bar_width)\n",
    "ax.set_xticklabels(x_values)\n",
    "ax.set_ylabel('Number of Countries')\n",
    "ax.set_title('Number of Countries where IT roles pay Less and More than the Median')\n",
    "\n",
    "# Show legend\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f5624-1d3a-4dd6-99bb-282958aa8945",
   "metadata": {},
   "source": [
    "### Plot interpretation\n",
    "From this bar chart we can observe that Software Engineering is a higher-paying role compared to Data Analysis in the context of our data.\n",
    "There is a stark contrast between the two professions in terms of compensation, with Software Engineers being better paid in the majority of countries. If you want a greater choice of where to live in the world whilst being well paid, choose to study Software Engineering rather than Data Analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7cbd-447c-4a47-92c0-31618b8b9feb",
   "metadata": {},
   "source": [
    "## Section 6.3 -  How Does Pay Parity Vary Across the World?\n",
    "_A Heatmap of Gender Pay Disparity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed77b8-4f8f-44e6-ade6-9be429d110cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data = df2.copy()\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "merged_data = world.merge(salary_data, how='left', left_on='name', right_on='WD_country_or_region')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "ax.set_title('Pay Parity Heatmap by Country')\n",
    "merged_data.plot(column='gender_pay_parity', cmap='RdYlGn', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=salary_data['gbp_converted_50th'].min(), vmax=salary_data['gbp_converted_50th'].max()))\n",
    "sm._A = []\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5349eb-af6b-4858-b141-703a5b0c5a17",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "\n",
    "## Analysis of Heatmap\n",
    "- \n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d72f08-29a4-4a1a-b413-ae9322f3d35a",
   "metadata": {},
   "source": [
    "## Section 6.4 - Is Pay Parity Correlated with Continent Population Density?\n",
    "_Comparing pay parity and population density per continent (Using ML)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d14ee-3227-4fee-b9bc-519aa2508c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1435bfb-d460-4a43-89e1-effcbf1065a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to replace the 'NaN' values for the continent of North America (another \"NA\" code pandas doesn't like)\n",
    "continent = 'continent'\n",
    "new_string = 'NA'\n",
    "data[continent].fillna(new_string, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f537427-bd52-48dc-8622-efdefdd5117f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fd23a-9503-4ebd-b443-5457a11097ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7170040-3eb9-4242-8df9-9eab93a1495e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adds a column with the pay parity category for each country, categorised 1=low, 2=medium, 3=high.\n",
    "def gender_pay_category(value):\n",
    "    if value < 0.6:\n",
    "        return 1\n",
    "    elif 0.6 <= value <= 0.7:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "data['gender_Pay_Num'] = data['gender_Pay'].apply(gender_pay_category)\n",
    "\n",
    "#Adds a column where each continent corresponds with a number so that the SVM model can calculate the metrics (MAE, MSE)\n",
    "continent_mapping = {'NA': 1, 'EU': 2, 'AS': 3, 'AF': 4, 'SA': 5, 'OC': 6}\n",
    "data['Continent_Num'] = data['continent'].map(continent_mapping)\n",
    "\n",
    "#Adds a column for population density (population/area) for each country.\n",
    "area=data['Area_in_km2']\n",
    "pop= data['population']\n",
    "density=pop/area\n",
    "    \n",
    "data['density'] = density\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f141fab-2d02-446a-a641-b7cc218431ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='gender_Pay', y='density', hue='continent', data=data, palette='Spectral')\n",
    "\n",
    "plt.xlabel('Pay Parity')\n",
    "plt.ylabel('Population Density')\n",
    "plt.title('Scatter Plot with Density, Pay Parity, and Continent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8d597-77bf-4660-a58a-bec014f48a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "x=np.where((data['density'] < 1000) & (data['gender_Pay'] > 0.5))\n",
    "df=data.iloc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20a789-4f2a-41db-bb72-1f18235f933e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='gender_Pay', y='density', hue='continent', data=df, palette='Spectral')\n",
    "\n",
    "plt.xlabel('Pay Parity')\n",
    "plt.ylabel('Population Density')\n",
    "plt.title('Scatter Plot with Density, Pay Parity, and Continent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf48351-c50b-4f57-9649-077284d0057a",
   "metadata": {},
   "source": [
    "### Plot Interpretation\n",
    "● Asia (AS): Asian countries show a wide range of pay parity and population densities. Some Asian countries have high population densities but span across the range of pay parity.\n",
    "\n",
    "● Europe (EU): European countries tend to cluster in the mid to high range of pay parity. Population density varies, with a few outliers having very high population density.\n",
    "\n",
    "● Africa (AF): African countries generally have lower pay parity, with population densities spread across the range.\n",
    "\n",
    "● South America (SA): South American data points are mostly in the lower half of population density with pay parity values mostly in the middle range.\n",
    "\n",
    "● Oceania (OC): There are fewer data points for Oceania, but they tend to have lower population densities and vary in pay parity.\n",
    "\n",
    "● North America (NA): North American countries appear to have a spread in population density but generally higher pay parity.\n",
    "\n",
    "There does not appear to be a clear correlation between population density and pay parity across the continents, however through other analysis methods we did discover firmer patterns.\n",
    "Developed regions (like North America and Europe) tend to have higher pay parity regardless of population density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f57a0-98cc-4705-aafe-bbf97be83ce1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "    <ul>\n",
    "         <li>The Code seems to go onto do something else here, we need a brief markdown cell to explain what happens in the next few cells</li>  \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d1655-01f3-4d45-8bf1-53b4134d20f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=df[['population', 'Continent_Num']]\n",
    "y=df['gender_Pay_Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce22f4-14fb-4a61-954f-fa0668ac4c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "svc = SVC(C=1.0, random_state=5, kernel='rbf')\n",
    " \n",
    "# Fit the model\n",
    "svc.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b2f0a-8f70-4ac4-ab65-9c56ff05d712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_predict = svc.predict(X_test_std)\n",
    " \n",
    "# Metrics to measure the performance of the model\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy score %.2f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "print('Mean Squared Error : ', mse)\n",
    "print('Mean Absolute Error : ', mae)\n",
    "print(\"Precision: \", metrics.precision_score(y_test, y_predict, average='micro'))\n",
    "print(\"Recall: \", metrics.recall_score(y_test, y_predict, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6f858-f7b8-4b4a-88d3-ae986afea2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analysis: SVM Model's prediction of Pay Parity category with two explanatory variables (population and continent number)\n",
    "\n",
    "- The model has an accuracy score of 0.68 which suggests that it is a fairly accurate prediction model.\n",
    "- The MSE and MAE are relatively which indicates that there were few errors made in predcting the pay parity category.\n",
    "- The precision and recall were 0.68 which is indicative of an above average prediction model. Precision measures the correct predictions made by the model, where the maximum is 1.0. Recall measures the relevant data points that were correctly identified by the model (the maximum is also 1.0).\n",
    "- Overall, this SVM model is a **good** prediction model.\n",
    "- (Categories for model evaluation: poor, average, good, great)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c7df2-5853-4c6c-8e2c-dc6e6ea2f058",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 6.5 - Is there is a relation between pay parity and the economic success of a country?\n",
    "_Evaluation of pay parity and IT salaries across countries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ad4c6-f7a9-45a3-bab1-1f2450014f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the median percentile_GBP_50 per country to use is as value of reference for the IT salaries for each country\n",
    "median_percentile_per_country = it_roles_df.groupby('country_name')['percentile_50_GBP'].median().reset_index()\n",
    "\n",
    "# Merge the median percentile_GBP_50 back into the original DataFrame\n",
    "it_roles_df = pd.merge(it_roles_df, median_percentile_per_country, on='country_name', how='left', suffixes=('', '_median'))\n",
    "\n",
    "# Data cleaning by deleting the values with a gender pay below 0.5 which belonged to Afghanistan and the median salaries above 70000\n",
    "filtered_gender_pay = it_roles_df[it_roles_df['gender_Pay'] > 0.5]\n",
    "cleaned_df = filtered_gender_pay[filtered_gender_pay['percentile_50_GBP_median']<70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55b6c2-179b-4308-930f-6a7fe41feee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cleaned_df['percentile_50_GBP_median'], cleaned_df['gender_Pay'], alpha=0.8)\n",
    "slope, intercept = np.polyfit(cleaned_df['percentile_50_GBP_median'], cleaned_df['gender_Pay'], 1)\n",
    "plt.plot(cleaned_df['percentile_50_GBP_median'], cleaned_df['percentile_50_GBP_median'] * slope + intercept, 'r', linewidth=2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentile_50_GBP Median')\n",
    "plt.ylabel('Pay Parity')\n",
    "plt.title('Scatter Plot of Median Percentile_GBP_50 vs. Gender_Pay per Country')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b82bbe-7c0f-4286-961c-c61956288704",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot interpretation\n",
    "After eliminating the data points considered outliers - pay parity data from Afghanistan and certain jobs getting paid above 70 K GBP - the above plot is the one we extract. We can see there is a slight positive correlation between the pay parity and the median salaries per country. \n",
    "There is a clustering of countries at the lower end of the income scale, indicating that a large number of countries with lower median incomes have varying gender pay ratios. \n",
    "From this we can hypothesise that while there is a relationship between the median income of a country and its gender pay gap, it is likely influenced by a variety of other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94886549-2274-4cc8-84f4-0aed193bc12d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 6.6 -  Is there a linear relationship between gender pay (dis)parity and cost of living metrics or population?\n",
    "_A OLS Regression analysis between WD_cost_living_rank, population, purchasing_power_index and gender_pay_parity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100e168-4679-4ec0-a683-d8329ef27666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ML analysis of pay parity using other variables such as cost index, PPI and WD cost of living rank.\n",
    "sal = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e00850-e783-4cce-afe5-7a1893159cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get one row for each country, a single row for each country is selected where the job id is 'data analyst'. \n",
    "da=np.where(sal['job_id']=='DATA-ANALYST')\n",
    "df2=sal.iloc[da]\n",
    "\n",
    "# Drops the 'WD_notes_special_regions' column as it is made up of NaN values for almost every country.\n",
    "df2.drop('WD_notes_special_regions', axis=1, inplace=True)\n",
    "\n",
    "# North America initials are 'NA' and these are being read as null values. To fix this, the rows with continents that are 'missing' \n",
    "#are assigned the initials 'NA'.\n",
    "north_america = 'NA'\n",
    "df2[continent].fillna(north_america, inplace=True)\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71966ce9-0f63-4a55-be2e-a14f54d5369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rows with null values are dropped.\n",
    "df2=df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f9569-b3a7-4ea9-9387-159ba920986a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pay parity is assigned a number from 1-3 (1=low, 2=medium, 3=high) so that the SVM model can use the variable, as it cannot work with continuous data.\n",
    "def gender_pay_category(value):\n",
    "    if value < 0.5:\n",
    "        return 1\n",
    "    elif 0.5 <= value <= 0.7:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "continent_mapping = {'NA': 1, 'EU': 2, 'AS': 3, 'AF': 4, 'SA': 5, 'OC': 6}\n",
    "df2['gender_Pay_Num'] = df2['gender_pay_parity'].apply(gender_pay_category)\n",
    "\n",
    "#A column for population density is added, calculated using the area_km2 and population columns.\n",
    "area=df2['area_km2']\n",
    "pop= df2['population']\n",
    "density=pop/area\n",
    "    \n",
    "df2['density'] = density\n",
    "\n",
    "#Continents are assigned a number so that the SVM model can use the variable, as it cannot work with strings or continuous data.\n",
    "continent_mapping = {'NA': 1, 'EU': 2, 'AS': 3, 'AF': 4, 'SA': 5, 'OC': 6}\n",
    "df2['Continent_Num'] = df2['continent'].map(continent_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2aa147-42f2-4f1d-b62a-a29b479c8c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=df2[['WD_cost_living_rank', 'population', 'purchasing_power_index']]\n",
    "y=df2['gender_pay_parity']\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e56422-d55c-404f-874f-9ed643d276a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#A constant is added to allow for a y-intercept that is not necessarily equal to zero.\n",
    "x1 = sm.add_constant(x)\n",
    "\n",
    "#Ordinary Least Squares(OLS) regression is used to generate a table which gives an extensive description about the regression results.\n",
    "result = sm.OLS(y,x1).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa29913-9d09-4407-80d8-67259cb1a6c1",
   "metadata": {},
   "source": [
    "### OLS Regression Analysis\n",
    "\n",
    "OLS Regression is used to evaluate the linear relationship between explanatory variables and the target variable. \n",
    "The results of the OLS regression show that there is a weak positive relationship between WD_cost_living_rank, population, purchasing_power_index and gender_pay_parity, as the R-squared value is 0.4. \n",
    "The P>t value of WD_cost_living_rank and population are significant as they are equal to/less than 0.1. A low p-value means  that a null-hypothesis (the idea that any observed difference between groups is due to chance) can be rejected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2de9a-d24e-4c66-982a-aaf1a45d92f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A visualisation of the relationship between WD cost of living rank and pay parity\n",
    "X=df2['WD_cost_living_rank']\n",
    "Y=df2['gender_pay_parity']\n",
    "\n",
    "plt.scatter(X,Y)\n",
    "slope, intercept = np.polyfit(X, Y, 1)\n",
    "plt.plot(X, X*slope + intercept, 'r')\n",
    "plt.xlabel('WD Cost of Living Rank', fontsize=20)\n",
    "plt.ylabel('Pay Parity', fontsize=20)\n",
    "plt.title('Scatterplot of Pay Parity and WD Cost of Living Rank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068bf85-e3e9-4dbb-8692-d6a5e972c861",
   "metadata": {},
   "source": [
    "### Plot interpretation\n",
    "\n",
    "- The scatterplot shows a negative corrolation between Pay Parity and WD Cost of Living Rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41af246-9234-4424-9d5b-c2201df0d244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A visualisation of the relationship between cost index and pay parity.\n",
    "X2=df2['cost_index']\n",
    "Y2=df2['gender_pay_parity']\n",
    "\n",
    "plt.scatter(X2,Y2)\n",
    "slope, intercept = np.polyfit(X2, Y2, 1)\n",
    "plt.plot(X2, X2*slope + intercept, 'r')\n",
    "\n",
    "plt.xlabel('Cost Index', fontsize=20)\n",
    "plt.ylabel('Pay Parity', fontsize=20)\n",
    "plt.title('Scatterplot of Pay Parity and Cost Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8bc66-7786-499c-bdb2-1b669e4519e0",
   "metadata": {},
   "source": [
    "### Plot Interpretation\n",
    "The scatterplot shows a slightly positive corrolation between Pay Parity and Cost Index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4000e-5556-49f5-bca9-f6c2d4d1f166",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "### Section 6.7 -  Is there a .... \n",
    "_A machine learning ...._\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7e33f-2ce0-484f-9462-b83c9ab2f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f00d4-9170-454b-bf59-0a63a576538c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=df2[['population', 'cost_index', 'Continent_Num']]\n",
    "y=df2['gender_Pay_Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770308e5-3a6c-4133-a0fc-d524b15b50e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)\n",
    "y_train.shape # (62,)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_train_std.shape # (62, 3)\n",
    "np.isnan(X_train_std).any() #False\n",
    "np.isinf(X_train_std).any() #False\n",
    "\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Using the rbf kernel produces the highest accurcy score\n",
    "svc = SVC(C=1.0, random_state=5, kernel='rbf')\n",
    " \n",
    "# Fits the model\n",
    "svc.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be969124-f255-4af3-8289-7a9edae7a382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_predict = svc.predict(X_test_std)\n",
    " \n",
    "# Metrics to measure the performance of the model\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy score %.2f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "print('Mean Squared Error : ', mse)\n",
    "print('Mean Absolute Error : ', mae)\n",
    "print(\"Precision:  %.2f\" %metrics.precision_score(y_test, y_predict, pos_label=3))\n",
    "print(\"Recall: %.2f\" %metrics.recall_score(y_test, y_predict,pos_label=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f50c3a-8c67-4933-af5a-9901ea36872f",
   "metadata": {},
   "source": [
    "### SVM Model to predict Pay Parity category with 'population', 'cost_index'and 'Continent_Num' as explanatory variables\n",
    "\n",
    "- The model has an accuracy score of 0.88 which suggests that it is an accurate prediction model.\n",
    "- The MSE and MAE are low which indicates that there were few errors made in predcting the pay parity category.\n",
    "- The precision and recall were 0.90 which is indicative of a great prediction model. Precision measures the correct predictions made by the model, where the maximum is 1.0. Recall measures the relevant data points that were correctly identified by the model (the maximum is also 1.0).\n",
    "- Overall this SVM model is a **great*** prediction model.\n",
    "- (Categories for model evaluation: poor, average, good, great)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee049ab-9708-4461-a7d2-a746ef4613be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A K-Nearest Neighbours(KNN) algorithm as a ML prediction model was developped. \n",
    "X=df2[['cost_index', 'purchasing_power_index']]\n",
    "y=df2['gender_Pay_Num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523d55b-575b-47c5-a75c-138a14295cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes=y_train\n",
    "\n",
    "# Combines the two explanatory variables to fit the model.\n",
    "data=list(zip(X_train.iloc[:,1], X_train.iloc[:,0]))\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(data, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407eb0f-c14b-440c-aebf-9037a55dfff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747fbe-330a-4366-b41b-f7eb13020604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metrics to measure the performance of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score %.2f\" %metrics.accuracy_score(y_test, y_pred))\n",
    "print('Mean Squared Error : ', mse)\n",
    "print('Mean Absolute Error : ', mae)\n",
    "print(\"Precision: %.2f\" %metrics.precision_score(y_test, y_pred, pos_label=3))\n",
    "print(\"Recall: %.2f\" %metrics.recall_score(y_test, y_pred,pos_label=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e9cee-5669-4c83-b94d-6d43c55f4375",
   "metadata": {},
   "source": [
    "## KNN Model to predict Pay Parity category with 'cost_index' and 'purchasing_power_index' as explanatory variables\n",
    "\n",
    "- The model has an accuracy score of 0.69 which suggests that it is a relatively accurate prediction model.\n",
    "- The MSE and MAE are fairly low which indicates that there were few errors made in predcting the pay parity category.\n",
    "- The precision was 0.86 and the recall was 0.60. The precision value is indicative of a great prediction model, however the recall is much lower. Precision measures the correct predictions made by the model, where the maximum is 1.0. Recall measures the relevant data points that were correctly identified by the model (the maximum is also 1.0).\n",
    "- Overall, this KNN model is a **good*** prediction model. \n",
    "- (Categories for model evaluation: poor, average, good, great)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ff7f9-a30c-4e08-9844-72f12452da5f",
   "metadata": {},
   "source": [
    "## Section 7 - Conclusions\n",
    "- Some of the salary data value require further analysis as the figures did not fall within a normal distribution\n",
    "- There were missing values for some countries in regards to the pay parity metric. The existing data seemed to be consistent except for 1 outlier, but it would be interesting to find a full dataset and re-evaluate it\n",
    "- There is a slight correlation between the economic success of a country relative to the country having a higher pay parity. This could be influenced by a complex interrelation of many factors.\n",
    "- Business Analysts, Data Scientists, IT Managers, Mobile Developers, Product Managers, Software Engineers and UX Designers stand out as occupations where the salary is very likely to be compensated above the country’s median salary- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7f595-06f9-4298-93ed-418f28def4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
