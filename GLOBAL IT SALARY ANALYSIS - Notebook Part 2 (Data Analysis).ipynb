{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26642f7e-b0b7-471a-8565-57f99747842d",
   "metadata": {},
   "source": [
    "# GLOBAL IT SALARY ANALYSIS (Git-Girls-Collective-7)  \n",
    "# - Notebook Part 2 (Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efdada-ad31-4449-bf4d-f5ce15fb391b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "This notebook is... (explain purpose (data analysis) relative to the previous notebook containing API python code)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73306aaa-c161-4d18-878d-42d81492aa9b",
   "metadata": {},
   "source": [
    "- [ ] Section: loading data\n",
    "- [ ] Section: cleaning data\n",
    "- [ ] Section: transforming data\n",
    "- [ ] Section: data analysis\n",
    "- [ ] Section: data visualisation\n",
    "- [ ] Section: data reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e7985-bb8d-4fc6-9085-9ea193a245e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Oranges cells are missing code / comments and require information / population\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f90b52-e63f-4c85-9652-3fd36e16e1df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: yellow; padding: 10px;\">\n",
    "Yellow cells are notes, to be deleted from final version\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d02cb-4a0d-49ad-8995-87b8f136b9b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: red; padding: 10px;\">\n",
    "Errors, big problems that need fixing\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0178d0-b9e1-4155-9e7f-a8c7923a4594",
   "metadata": {},
   "source": [
    "# Section 1 - Transforming API call Data into DataFrames\n",
    "# _(A loading/cleaning/transforming section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd072d-e5dc-43f6-8516-b3c536dbc5e5",
   "metadata": {},
   "source": [
    "### Notebook Part 2 File Requirements\n",
    "The first three files are the raw datasets we gathered:\n",
    "* **cost_living_w_codes.csv**\n",
    "* **Gender Pay Gap.csv**\n",
    "* **country_codes.sql** (this must be initialised as a DB in MySQL workbench)\n",
    "* **config.py** - with completed MySQL username, password and hostname\n",
    "* **output_gbp_salaries_23-11-29_10-55.csv**\n",
    "* \n",
    "This final file is the output of our main.py Python code, which makes several API calls to Teleport for country & salary data, and one to exchangerate-api.com for currency conversion rates. It contains the fields:  \n",
    "\\* country codes * local currency code * salaries in local currency (25th/50th/75th) * conversion rate to gbp * gbp converted salaries  \n",
    "!! Our code is designed to give the final csv file a timestamped (so unique) name in the format output_gbp_salaries_{timestamp}, to facilitate version control and data integrity.\n",
    "This is the 'frozen' snapshot of API call data upon which we based our project, timestamped 23-11-29_10-55. If you are running our Python code and Jupyter Notebook to generate a _fresh_ dataset, the csv name will be your equivilent timestamped filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4a7eb2-7a01-4f54-be85-4c2a9a1e2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sqlalchemy # if required\n",
    "# pip install openpyxl # if required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555d957-a6f4-45b6-af27-d817e020bb1d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Instructor Note: Please ensure that you have upto date versions of X, X, X modules (versions... ) otherwise the cell below which communiates with the SQL database will produce errors! \n",
    "    (Obviously remove the orange once this cell content is completed1)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677041a-7fa0-4302-976b-c306a9104105",
   "metadata": {},
   "source": [
    "<div style=\"background-color: yellow; padding: 10px;\">\n",
    "    Remove lines which create new .csv files at every stage, once we are happy with the code and output. These were for testing only.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3d13a8-b9c3-4088-bc09-c4d005cbdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598eeca-3da1-4f32-b8e4-6e8828df6a08",
   "metadata": {},
   "source": [
    "Load the timestamped csv file which has all the combined data from our various API calls, into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b000b88-2cfc-483c-99ff-528c696eea9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_id                                0\n",
       "job_title                             0\n",
       "salary_percentiles_percentile_25      0\n",
       "salary_percentiles_percentile_50      0\n",
       "salary_percentiles_percentile_75      0\n",
       "iso_alpha2                            0\n",
       "currency_code                         0\n",
       "local_to_gbp_rates                  156\n",
       "gbp_converted_25th                  156\n",
       "gbp_converted_50th                  156\n",
       "gbp_converted_75th                  156\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_sal_df = pd.read_csv(\"data/output_gbp_salaries_23-11-29_10-55.csv\") # match the filename to the timestamped csv you are processing\n",
    "api_sal_df.isnull().sum()\n",
    "\n",
    "# Correct the missing iso_alpha2, Namibia \"NA\"\n",
    "null_iso_alpha2 = api_sal_df[api_sal_df['iso_alpha2'].isnull()].copy()\n",
    "null_iso_alpha2 # It's Namibia, index 6552-6603\n",
    "api_sal_df.loc[6552:6603, 'iso_alpha2',] = 'NA'\n",
    "api_sal_df.isnull().sum() # fixed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec42e7b-a8ca-4064-8eae-fdea67a84f35",
   "metadata": {},
   "source": [
    "### Import data from country_codes.sql, and merge with the API DataFrame   \n",
    "api_sal_df at this point identifies countries only by iso_alpha2 codes, not their names, and we also want to import area and population data from country_codes.sql file.  \n",
    "The following code imports data from the MySQL table and converts it into a pandas DF called \"countries_df\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f9cf5-af97-45cb-98cc-865ddb2222d9",
   "metadata": {},
   "source": [
    "!! You will need to have run country_codes.sql in MySQL to have created the \"database countries_db\" and the \"table country_codes\"\n",
    "!! You must also supply your MySQL login credentials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc730c2-8b0d-495f-9571-ed96bc1d04e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# mysql.connector or pymysql don't work with Jupyter / MySQl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f7a41-b949-4082-826e-d78222a3e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_codes.sql information is in an .sql table. Need to convert sql > db in order for pandas to turn into into DF. Then convert to csv\n",
    "from sqlalchemy import create_engine\n",
    "# Import configuration variables from config.py\n",
    "from config import DATABASE_USER, DATABASE_PASSWORD, DATABASE_HOST\n",
    "\n",
    "# MySQl database connection details\n",
    "username = DATABASE_USER\n",
    "password = DATABASE_PASSWORD\n",
    "host = DATABASE_HOST\n",
    "database = 'countries_db'\n",
    "\n",
    "# Creates a database engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\")\n",
    "print(engine)\n",
    "\n",
    "# The SQL query to get all info from table named country_codes\n",
    "query = \"SELECT * FROM country_codes\"  \n",
    "\n",
    "# Use Pandas to load data into a DataFrame\n",
    "countries_df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbb204-cc85-4c7e-8f6d-cdfd37e31a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df.head() # preview the imported DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d70985-dfc8-444f-b6ee-8def4f74b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_df.to_csv(\"country_codes.csv\", encoding='utf-8', index=False) # intermediate backup of DF to .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0aba3-8042-4efb-93b1-b9dfcfc052b1",
   "metadata": {},
   "source": [
    "Join api_sal_df to countries_df. This is an inner join because we don't need info about countries for which we have no salary data (that being the focus of our data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb42efb-443c-4d37-ad96-3cae0c6d01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join api_sal_df to country_codes df on iso_alpha2. The inner join excludes any countries from country_codes.sql for which we don't have salary data\n",
    "biggie_dfv1 = pd.merge(api_sal_df, countries_df, on='iso_alpha2', how='inner')\n",
    "# biggie_dfv1.to_csv(\"sal_and_country.csv\", index=False) # intermediate backup of DF to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7221dd-83a1-4140-a0fc-514140dc6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv1.head() # preview the merged DF. [10244 rows x 19 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc51f23-28e8-4c35-87fc-cd098c13846e",
   "metadata": {},
   "source": [
    "### Join gender pay data (column) with our growing DF. Read to DF, clean, merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48122caa-c34a-4b7d-a6c2-cc92bbc18412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in gender_pay_parity.csv column into a new DataFrame\n",
    "gender_df = pd.read_csv(\"data/Gender Pay Gap.csv\")\n",
    "gender_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfa80c-aeeb-4873-8ae4-80602335b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning: rename coumns Country to country / Gender_Pay_Parity gender_pay_parity to facilitate merge\n",
    "gender_df.rename(columns={'Country': 'country', 'Gender_Pay_Parity':'gender_pay_parity'}, inplace=True)\n",
    "gender_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d4eb9-62e5-4d95-84f5-d42829d0df0f",
   "metadata": {},
   "source": [
    "### Joining our growing DF with gender_pay_parity column data  \n",
    "This is an _outer_ join because we are joining on the 'country'(name) column rather than a controlled, standardised column like iso_alpha2. If this wasn't an outer join, we may miss data which doesn't match due to minute different spellings of names. This means the data needs to be reviewed later for duplicate countries (same country but slightly different names/different characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2154a9-11c8-4a35-8a54-f29ad362722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv2 = pd.merge(biggie_dfv1, gender_df, on='country', how='outer') \n",
    "# biggie_dfv2.to_csv(\"sal_and_country_and_gender.csv\", index=False) # intermediate backup of DF to .csv file\n",
    "biggie_dfv2.head(10) \n",
    "biggie_dfv2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51ea1c-9144-49ab-a24b-3e89424cd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_countries = biggie_dfv2['country'].unique().tolist()\n",
    "count_countries = len(unique_countries)\n",
    "count_countries # 216"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccd53d-ae21-49d8-91e4-fa960bd940c4",
   "metadata": {},
   "source": [
    "### Final join for our growing DF with columns from cost_of_living data (from WorldData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abad1d-607e-4075-8a07-64042bd87943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, load the cost_of_living.csv data into a DF\n",
    "cost_living_df = pd.read_csv(\"data/cost_living_w_codes.csv\")\n",
    "cost_living_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337f4ae-39dd-4182-9c0e-d9beece3bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then merge cost of living DF with our current main DF, to create a final superlarge DF containing all data\n",
    "biggie_dfv3 = pd.merge(biggie_dfv2, cost_living_df, on=\"iso_alpha2\", how=\"left\")\n",
    "biggie_dfv3.head(10)\n",
    "biggie_dfv3.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547fbe0-7eb9-40d5-a488-260210b4123f",
   "metadata": {},
   "source": [
    "### Cleaning: rename the column 'rank' from cost_of_living.csv to WD_cost_living_rank for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab38c3f-faa4-4844-9ddc-119c4c97aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv3.rename(columns={'rank': 'WD_cost_living_rank', 'country_or_region': 'WD_country_or_region'}, inplace=True)\n",
    "biggie_dfv3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849eb4d-9c27-411a-810d-75ed2fe414bd",
   "metadata": {},
   "source": [
    "### Cleaning: rename local currency columns (from Teleport API) to make name shorter and clearer that values are in local currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f979d-8bfa-4133-b918-dd8cecd80e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie_dfv3.rename(columns={'salary_percentiles_percentile_25': 'salary_local_25th_pcl', 'salary_percentiles_percentile_50': 'salary_local_50th_pcl', 'salary_percentiles_percentile_75': 'salary_local_75th_pcl', 'monthly_income_USD' : 'WD_monthly_income_USD', 'notes_special_regions' : 'WD_notes_special_regions'}, inplace=True)\n",
    "# biggie_dfv3.to_csv(\"sal_country_gender_costliving.csv\", index=False) # intermediate backup of DF to .csv file\n",
    "biggie_dfv3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4b1b7-4d98-4366-9bae-edbfdd325299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column for GBP monthly income\n",
    "def usd_monthly_income_to_GBP(USD_monthly_income):\n",
    "    if isinstance(USD_monthly_income, str) and USD_monthly_income.strip():\n",
    "        USD_num_only = USD_monthly_income.replace(\",\", \"\").replace(\" USD\",\"\").strip()\n",
    "    else:\n",
    "        return None # if not a string. note, no print message, should just skip\n",
    "    \n",
    "    try: \n",
    "        USD_num_only = float(USD_num_only)\n",
    "    except ValueError:\n",
    "        print(\"Error converting string to int\") \n",
    "        return None\n",
    "    GBP_monthly_income = int(USD_num_only * 1.267997)\n",
    "    return GBP_monthly_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbfc89-1fde-4683-9202-b3f770c6ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with country average monthly salary (WorldData) converted to GBP in new column \n",
    "final_df = biggie_dfv3.copy()\n",
    "final_df['WD_monthly_income_GBP'] = final_df['WD_monthly_income_USD'].apply(usd_monthly_income_to_GBP)\n",
    "# reordering the columns for the final DF\n",
    "final_df = final_df[\n",
    "    [\n",
    "        'iso_alpha2',\n",
    "        'country',\n",
    "        'currency_code',\n",
    "        'local_to_gbp_rates',\n",
    "        'job_id',\n",
    "        'job_title',\n",
    "        'salary_local_25th_pcl',\n",
    "        'gbp_converted_25th',\n",
    "        'salary_local_50th_pcl',\n",
    "        'gbp_converted_50th',\n",
    "        'salary_local_75th_pcl',\n",
    "        'gbp_converted_75th',\n",
    "        'WD_country_or_region',\n",
    "        'WD_cost_living_rank',\n",
    "        'WD_monthly_income_USD',\n",
    "        'WD_monthly_income_GBP',\n",
    "        'WD_notes_special_regions',\n",
    "        'cost_index',\n",
    "        'purchasing_power_index',\n",
    "        'gender_pay_parity',\n",
    "        'iso_alpha3',\n",
    "        'iso_numeric',\n",
    "        'fips',\n",
    "        'capital',\n",
    "        'area_km2',\n",
    "        'population',\n",
    "        'continent'\n",
    "    ]\n",
    "]\n",
    "final_df.to_csv(\"final_df_inc_GBP_monthly.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373786e4-59ca-49bc-b6d1-a14b21afedc0",
   "metadata": {},
   "source": [
    "# Section 2 - Making a MySQL Database\n",
    "# _A loading/cleaning/transforming section_\n",
    "Step 1: Splitting the large DF into 4 refined DataFrames, which were used to populate MySQL Database tables. Only run this cell if you want individual copies of the csvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd14fe-e4f5-4818-bfa5-17e02581fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create reduced DFs to serve as sql table starters\n",
    "# countries_sql_table_df = final_df[['iso_alpha2', 'country', 'capital', 'continent', 'area_km2', 'population','gender_pay_parity']].drop_duplicates(subset='iso_alpha2').copy() # excluded 'iso_alpha3', 'iso_numeric' 'fips' \n",
    "# countries_sql_table_df.to_csv(\"countries_data_from_final_df.csv\", index=False)\n",
    "\n",
    "# cost_of_living_sql_table_df = final_df[['iso_alpha2', 'WD_country_or_region','WD_notes_special_regions','WD_cost_living_rank', 'WD_monthly_income_USD', 'WD_monthly_income_GBP', 'cost_index', 'purchasing_power_index']].drop_duplicates(subset='iso_alpha2').copy()\n",
    "# cost_of_living_sql_table_df.to_csv(\"cost_of_living_data_from_final_df.csv\", index=False)\n",
    "\n",
    "# salaries_sql_table_df = final_df[['iso_alpha2',  'job_id', 'job_title', 'salary_local_25th_pcl', 'salary_local_50th_pcl', 'salary_local_75th_pcl', 'currency_code', 'local_to_gbp_rates','gbp_converted_25th','gbp_converted_50th', 'gbp_converted_75th']].copy()\n",
    "# salaries_sql_table_df.to_csv(\"salaries_data_from_final_df.csv\", index=False)\n",
    "\n",
    "# job_sql_table_df = final_df[['job_id', 'job_title']].drop_duplicates(subset='job_id').copy()\n",
    "# job_sql_table_df.to_csv(\"job_data_from_final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d0cc4-7fe5-4ebe-a5b3-59b01e232df9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: yellow; padding: 10px;\">\n",
    "    Cost of living data from WorldData hasn't made it's way into the SQL database. If we don't end up using it at all, we need to go back through Section 1 and remove the steps, references and comments related to this dataset.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b6182-217c-438b-9c04-af3c03e809ed",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "    <ul>\n",
    "        <li>Need commentary here about the steps taken the SQL Database was constructed - DONE </li> \n",
    "        <li>Need code (or a reference to an external file, if it is too long to put into the Jupyter NB) which will allow instructor to construct SQL Database/</li>  \n",
    "        <li>The SQL tables contain the 4 fixes NP identified (Namibia, and three outdated currency codes). We went back and fixed these errors at source in the Python code, so fresh datasets won't have these problems, however we had already frozen the versions of our data used when it was imported into the SQL database. This means that we need to provide some code that can be run on the datasets we used, to fix the problems.</li>\n",
    "        <li>The code below also starts it's analysis from Job Insights.xlsx. We need to provide code or an explanation as to how we got this file from the SQL database.- DONE</li>\n",
    "        <li>How were the gbp converted salaries calculated for VES, MRU, BYN? The gbp salaries were blank in the original DFs, as there wasn't a match between Teleport and the currency API. Was it manually done from plugging in the the currency conversion rates from the json? </li>\n",
    "    </ul>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cda717-346e-43bf-8d3d-39e50d1490eb",
   "metadata": {},
   "source": [
    "### **The process of creating the SQL database involved the following steps:**\n",
    "\n",
    "1. We began by utilising API data along with country codes and gender disparity information to construct a comprehensive spreadsheet. This spreadsheet had distinct sheets representing both the SQL structure tables and sample data.\n",
    "2. To enhance clarity, we organised the data into various sheets, facilitating the visualization of SQL tables alongside their corresponding sample data.\n",
    "3. Subsequently, an Entity-Relationship (ER) diagram was developed to provide a visual representation of the tables, columns, and data types. Initially, around 10 different tables were conceptualised based on the original datasets.\n",
    "4. In a collaborative group meeting, the ER diagram was presented, fostering discussions that led to the finalisation of the SQL tables.\n",
    "5. The conclusive version of the ER diagram incorporated essential details such as primary and foreign keys, ensuring the integrity of the relational database.\n",
    "6. Following this, we returned to the Excel spreadsheet to normalize the database. Three distinct sheets were created, each intended to serve as an SQL table.\n",
    "7. We meticulously transferred the data into the corresponding columns, and to ensure accuracy, references such as ISO alpha-2 and alpha-3 codes were cross-referenced with the additional data in adjacent columns.\n",
    "8. To streamline the process, we used the V-LOOKUP formulas. These formulas were instrumental in accurately identifying country codes and gender disparities for each country, thereby enhancing the overall coherence and reliability of the dataset\n",
    "\n",
    "### Countries Table:\n",
    " \n",
    "1. Out of the 250 countries considered, gender pay disparity data was available for 136 countries. For the remaining countries, 'NULL' values were populated in the SQL database to replace the absence of this information.\n",
    "2. . Four specific **countries—Antarctica**, **Bouvet Island**, **Heard and McDonald Islands**, and **U.S. Outlying Islands** had '0' as the recorded value for their population. Additionally, for U.S. Outlying Islands, there was a recorded '0' for the area in square kilometer\n",
    "3. . Antarctica presented a unique case with 'N/A' as the recorded value for Currency Code and this was treated as 'NULL' in the SQL database.Upon further investigation, it was revealed that Antarctica is a continent without a native population. However, it is home to a transient population of scientists and support staff from various countries who live and work in research stations. Given the absence of adefined population figure, '0' was used to represent this valu .\n",
    "\n",
    "4. **Bouvet Island**, situated in the South Atlantic Ocean and under Norwegian dependency, is uninhabited,justifying the '0' population value. Similarly, **Heard and McDonald Islands**, administered by Australia for scientific research purposes, also have no permanent population7\n",
    "5. **U.S. Outlying Islands**, a group of nine insular areas outside the 50 states and the District of Columbia, exhibited varied population statuses—some with small populations and others uninhabited. This diversity in population characteristics was reflected in the SQL database for accurate representation5\n",
    "6. We have the gender pay for **Congo Republic** but not for **DR Congo**. after further research, we realised \n",
    "that, **Democratic Republic of the Congo (DRC)** is the larger of the two countries and is often referred to \n",
    "simply as the Congo and Its capital is Kinshasa.Whereas **Republic of the Congo** is a separate, neighbouring country sometimes referred to as Congo-Brazzaville to distinguish it from the Democratic Republic of the Congo.\n",
    "\n",
    "### Job Table:\n",
    "1. We didn’t experience any issues with this tables as the data was similar to the other tables.\n",
    "\n",
    "### Salaries Table:\n",
    "1. Incorrect values were displayed for an accountant in Ghana(GH). Please see below the incorrect values displaye\n",
    "2. **|1|GH|ACCOUNTANT|Accountant|1|£0.065696677|2|£0.131393354|4|£0.262786709**O3\n",
    "2. We were missing the converted salary rate for the ‘percentile_25_GBP, percentile_50_GBP and percentile_75_GBP’. These values were missing for Iso alpha 2 - ‘VE’, ‘MR’ and ‘BY4\n",
    "3. The API code was amended again to retrieve these values whilst the API code was updated, we notice that the currency code was incorrect hence why these values were not pulled through5\n",
    "4. The country code and the salary values were updated in excel for each relevant country before inputting it in SQL.\n",
    "\n",
    "\n",
    "### Exporting the data from SQL tables into Job Insights.xlsx involved the following steps:\n",
    "1. We decided that it would be great to have a spreadsheet where all the tables data were displayed in one spreadsheet as it would be easier to read different sheets on jupyter notebook.\n",
    "2. **Countries Table** - We wanted to be able to export this data from SQL as a CSV file and this data was later pasted into **Job Insights.xlsx**.\n",
    "3. **Salaries & Job Table** - Exporting data from these tables were slightly difficult as SQL only allowed us to export 1000 rows of data. The salaries and job tables has about 10,000 rows of data. We export the data in order by numbering each CSV file. The data from the CSV files were then pasted into **Job Insights.xlsx**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1221f-a19e-4e4f-b9a7-c592ec5e7d82",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "    Note:\n",
    "    During the SQL Database creation, 3 currency code mismatches were uncovered, as was a problematic iso_alpha2 country code (Namibia, NA).\n",
    "    We went back and corrected the currency codes within our API call python code (extinguishing the currency code mismatch at source, before it found it's way into our data) and we have corrected Namibia's 'blank' iso_alpha2 at the first dataframe creation stage. \n",
    "    <br>\n",
    "    Screenshots that were here should go into the project documentation instead. \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed51137-3bd1-4cc1-b7f3-cfbcbadf0d2a",
   "metadata": {},
   "source": [
    "# Section 3 - Loading data from (xlxs file exported from) MySQL tables into DataFrames for analysis. _(A loading/cleaning/transforming section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c462c9-8949-4124-8b1a-5782b721ca3a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: yellow; padding: 10px;\">\n",
    "This section of code requires the file 'Job Insights.xlsx' to be in the directory. Or we need to supply some code above which generates this xlsx file from the SQL database.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c9320-4496-4f0c-b136-86612bcd8ed7",
   "metadata": {},
   "source": [
    "### Notebook File Requirements for Section 3\n",
    "* **Job Insights.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4059643-69c6-40b5-a982-5929363ad95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries table only\n",
    "SQL_countries_df = pd.read_excel('data/Job Insights.xlsx', sheet_name = 'Countries')\n",
    "SQL_countries_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fea441-5223-4bb2-9b14-9884bed2da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the missing iso_alpha2. This was a PK in the SQL DB, so must have been present but lost in conversion to DF\n",
    "null_iso_alpha2 = SQL_countries_df[SQL_countries_df['iso_alpha2'].isnull()]\n",
    "null_iso_alpha2 # It's Namibia, index 159\n",
    "SQL_countries_df.loc[159, 'iso_alpha2'] = 'NA'\n",
    "SQL_countries_df.isnull().sum() # fixed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5502732-760f-45b7-b886-caf2923763f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_currency_code = SQL_countries_df[SQL_countries_df['currency_code'].isnull()]\n",
    "null_currency_code  # It's Antarctica, index 8\n",
    "# Replace DF with DF keeping only the rows where iso_alpha2 is not 'AQ'\n",
    "SQL_countries_df = SQL_countries_df[SQL_countries_df['iso_alpha2'] != 'AQ']\n",
    "SQL_countries_df.isnull().sum()  # fixed. So 113 countries with no gender_Pay info, and 41 without a continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbdba2-50c1-477a-9086-264ca1da5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salaries table only\n",
    "SQL_salaries_df = pd.read_excel('data/Job Insights.xlsx', sheet_name = 'Salaries')\n",
    "SQL_salaries_df.isnull().sum() # Namibia causing problems again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96834d-1f64-446f-9658-0395f0d008c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Namibia's \"NA\" iso_alpha2 code from NaN values (lost in DF creation) to the country code NA\n",
    "SQL_salaries_df.loc[6553:6604, 'iso_alpha2'] = 'NA'\n",
    "SQL_salaries_df.iloc[6553:6604]\n",
    "SQL_salaries_df.isnull().sum() # all 0, so all columns complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32920cb-d70c-45be-bdae-a048a26018d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_all_df = pd.merge(SQL_salaries_df, SQL_countries_df, on='iso_alpha2', how='inner')\n",
    "SQL_all_df.to_excel(\"SQL_all_data_joined_inner.xlsx\", index=True)\n",
    "SQL_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74d745-fc3d-4257-88ae-2fcb75a5c0e0",
   "metadata": {},
   "source": [
    "# Section X - Basic Analysis _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e13011-f697-4a7e-8661-d4e202d21ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis\n",
    "countries_list = SQL_all_df['iso_alpha2'].unique().tolist() \n",
    "countries_count = len(countries_list) # 198 unique country codes\n",
    "countries_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6306f0c-3126-421b-b219-cf1a8ba9d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list = SQL_all_df['Job_id'].unique().tolist() \n",
    "jobs_count = len(jobs_list) #  52 unique jobs\n",
    "jobs_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bffb70-f7a3-4afa-948e-0735ca937b16",
   "metadata": {},
   "source": [
    "# Section X - Heatmap _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f93c8-ee4c-4183-9319-3ca505a3d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the null values\n",
    "plt.figure(figsize = (8,6))\n",
    "ax = sns.heatmap(SQL_all_df.isnull(), cmap = 'viridis', cbar=False, annot=False)\n",
    "\n",
    "# Absolute totals of missing values for annotations for columns\n",
    "tot_gender_pay_missing = SQL_all_df['gender_Pay'].isnull().sum()\n",
    "tot_continent_missing = SQL_all_df['continent'].isnull().sum()\n",
    "\n",
    "# Positioning the absolute total labels\n",
    "n = list(SQL_all_df.columns).index('gender_Pay')\n",
    "m = list(SQL_all_df.columns).index('continent')\n",
    "ax.text(n+0.5, -0.5, tot_gender_pay_missing, ha='center', va='bottom', color='green', fontsize=15) \n",
    "ax.text(m+0.5, -0.5, tot_continent_missing, ha='center', va='bottom', color='green', fontsize=15) \n",
    "\n",
    "# legend\n",
    "plt.text(0.02, 0.75, 'Green Text = Total Missing Values', color='green', fontsize=15, transform=ax.transAxes)\n",
    "\n",
    "ax.set_title('Missing Data Heatmap', pad=20, fontsize=15)\n",
    "plt.ylabel('Entries Missing')\n",
    "plt.xlabel('Data Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf21b1a-b72f-44bb-ba04-de72165c32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_gender_pay = int(tot_gender_pay_missing / jobs_count)\n",
    "missing_gender_pay # 69\n",
    "print(f\"There are {missing_gender_pay} countries without gender pay parity figures out of {countries_count}, which is {(missing_gender_pay/countries_count)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b67aac-7918-4528-a6b7-ba360931861d",
   "metadata": {},
   "source": [
    "## **Analysis**\n",
    "* Due to our data processing choices and the nature of our joins, our data is fully saturated for the series we are most interested in (country, salaries in GBP).\n",
    "* Missing continent data could be fairly easily updated, however given the time alloted for our analysis we are unlikely to be able to branch out into broadeer factors like geography, continent etc.\n",
    "* We do not have full gender pay parity figures for all countries, only 66% of our total dataset. However, that does still leave 129 countries with complete data, which means it is likely to be worth analysing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbd4cf-31e8-4bde-bbc6-bbbaa443c883",
   "metadata": {},
   "source": [
    "# Section X - Analysis of Extreme Values _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2085b-cc6a-4c18-b359-6e039e915121",
   "metadata": {},
   "source": [
    "To be continued... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e2ade-f18b-44f7-860c-10ed64a76238",
   "metadata": {},
   "source": [
    "# Section X - Adaptation of Questions to Extreme Values _(an Analysis Section)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dcc20-0893-4cc2-8aa1-64cb83276e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for full integrity of DataFrame used for next analysis\n",
    "SQL_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc8f4c-c855-4db7-80db-58e7c9bb255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a single job role (data analyst) and looks at it's GBP converted salaries across all countries\n",
    "X = np.where(SQL_all_df['Job_id'] == 'DATA-ANALYST')\n",
    "df = SQL_all_df.iloc[X]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb439a-87ec-48b6-84c6-1538b174e0b9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of what this table demonstrates (i.e. problematic variation in salaries for same job role across countries, even  though converted to GBP, and even after you roughly account for varying costs of living\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa7a6f-9f90-4df1-abf5-61b056546c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This graph needs a title, it's a boxplot of the 50th percentile salaries for Data Analyst role\n",
    "sns.boxplot(df['percentile_50_GBP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50310a-39e0-45ce-9f6d-0b01e4f7b453",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of boxplot needed. Not just the variation, but the concentration of salaries in the extremely low (too low to be realistic) salary range\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded2974-bae1-408a-8f60-096efe4b0c65",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "    If the group decides to acknowledge the variation explicitly and what this means for our data!... Then...\n",
    "    <br>\n",
    "    SK to include here section about external research done regarding country minimum wage etc, referencing email sent to Teleport possibly\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbf115-652c-47b2-bf2f-2a6df1989b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"percentile_50_GBP\"], kde=True, stat=\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e13ccc-0c4b-4dd4-9fbe-2a2afc78a0d1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of histogram needed\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b3128-1724-4e31-962a-4352d37a13ee",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "We need a cell that explains how we have adapted our analysis, based on the above demonstration of extreme values, to show something meaningful from what we have (based upon the assumption that WITHIN COUNTRIES salary ranges are realistic/consistent)... \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faff3a-c8f9-45ff-9418-b7edc9ffcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the IT roles the group decided we wanted to focus our analysis on\n",
    "it_roles = ['BUSINESS-ANALYST', 'DATA-ANALYST', 'DATA-SCIENTIST', 'IT-MANAGER', 'MOBILE-DEVELOPER', 'PRODUCT-MANAGER', 'QA-ENGINEER', 'SOFTWARE-ENGINEER', 'UX-DESIGNER', 'WEB-DESIGNER', 'WEB-DEVELOPER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366188a4-d9e8-4afb-bc46-28f71434fa48",
   "metadata": {},
   "source": [
    "The following cell defines and calls a function which takes in a list of interesting job_ids (IT roles) and produces bar charts for each role showing how the salary for that role compares to the median 50th percentile GBP salary across all the countries for which we have data. This is intended to give an idea of how well paid that role is, relative to others, globally, whilst respecting the limitations we found within our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27ce7a-fbcf-42ef-9cd9-f530d2346be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_relative(role, data, ax):\n",
    "        # Plotting for each role\n",
    "        iso_alpha2 = data['iso_alpha2'].unique() # store unique iso_alpha2 codes in df\n",
    "        more, less, equal = [], [], []\n",
    "    \n",
    "        # Cycles through all countries\n",
    "        for x in iso_alpha2: # for every unique item in alpha2 list...\n",
    "            country_data = data[data['iso_alpha2'] == x]\n",
    "            med = country_data['percentile_50_GBP'].median() # take the median of 50pc salary in GBP\n",
    "            r_med = math.ceil(med*100)/100\n",
    "    \n",
    "            job = country_data[country_data['Job_id'] == role]\n",
    "            if not job.empty:\n",
    "                salary = job['percentile_50_GBP'].iloc[0]\n",
    "                r_salary = math.ceil(salary * 100) / 100 \n",
    "    \n",
    "                if r_salary < r_med:\n",
    "                    less.append(salary)\n",
    "                elif r_salary > r_med:\n",
    "                    more.append(salary)\n",
    "                else:\n",
    "                    equal.append(salary)        \n",
    "    \n",
    "        # Plotting of individual chart\n",
    "        num_more = len(more)\n",
    "        num_equal = len(equal)\n",
    "        num_less = len(less)\n",
    "    \n",
    "        ax.bar('Less than Median', num_less, color='blue', label='Less than Median')\n",
    "        ax.bar('Equal to Median', num_equal, color='green', label='Equal to Median')\n",
    "        ax.bar('More than Median', num_more,  color='orange', label='More than Median')\n",
    "        \n",
    "        ax.set_ylabel('Number of Countries')\n",
    "        ax.set_title(f\"Number of Countries where {role.replace('-',' ').title()}s \\n are paid Less / More than Median Country Salary\")\n",
    "        ax.legend()\n",
    "    \n",
    "# Create a figure with 6 rows and 2 columns of subplots\n",
    "fig, axes = plt.subplots(nrows=6, ncols=2, figsize=(15, 30))\n",
    "fig.tight_layout(pad=5.0) # sets padding between plots\n",
    "\n",
    "# Iterate over roles and plot\n",
    "for i, role in enumerate(it_roles):\n",
    "    row = i // 2 # Determine row: 0 for 1st two roles, 1 for next two etc\n",
    "    col = i % 2 # Deterimines column: 0 for first role in pair, 1 for second\n",
    "    ax = axes[row, col] # targets the subplot\n",
    "    role_relative(role, SQL_all_df, axes[row, col])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4cadb-b4d5-4375-9c17-1c756ae6223f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of charts needed. This is probably our main analysis section.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2abe7-d200-4a5c-9601-db77b034ec50",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "The following function demonstrates... (Compares data analyst and software engineer)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77102540-98d6-42ce-b60d-fde1e80b36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ['Data Analyst', 'Software Engineer']\n",
    "data1 = [173, 16]\n",
    "data2 = [30, 182]\n",
    "\n",
    "bar_width = 0.25\n",
    "\n",
    "positions1 = np.arange(len(x_values))\n",
    "positions2 = positions1 + bar_width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(positions1, data1, width=bar_width, label='Less than Median')\n",
    "ax.bar(positions2, data2, width=bar_width, label='More than Median')\n",
    "\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(positions1 + bar_width)\n",
    "ax.set_xticklabels(x_values)\n",
    "ax.set_ylabel('Number of Countries')\n",
    "ax.set_title('Number of Countries where IT roles pay Less and More than the Median')\n",
    "\n",
    "# Show legend\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e9cee-5669-4c83-b94d-6d43c55f4375",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px;\">\n",
    "Analysis of above chart\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73802c1-481e-4595-bcda-55077b23eefa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: red; padding: 10px;\">\n",
    "# CELLS BEYOND THIS POINT NOT RUN, just notes / ideas!\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa1e76-7899-4fe1-9f32-c109a3a73664",
   "metadata": {},
   "source": [
    "### Cleaning task: Need to clean these names, if they are still in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9322749-89b3-4512-ae84-cb00273858bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CuraÃ§ao\tWillemstad\n",
    "Ã…land\tMariehamn\n",
    "Saint BarthÃ©lemy\tGustavia\n",
    "RÃ©union\tSaint-Denis\n",
    "SÃ£o TomÃ© and PrÃ­ncipe\n",
    "\n",
    "# Define the dodgy characters to search for\n",
    "dodgy_character = 'Ã' AND .... \n",
    "\n",
    "# Create a boolean mask to identify rows with the dodgy character in any field\n",
    "mask = df.apply(lambda x: x.str.contains(dodgy_character)).any(axis=1)\n",
    "\n",
    "# Get the rows with the dodgy character\n",
    "dodgy_rows = df[mask]\n",
    "\n",
    "# Print or process the dodgy rows\n",
    "print(dodgy_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
